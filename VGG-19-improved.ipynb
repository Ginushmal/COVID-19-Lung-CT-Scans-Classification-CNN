{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-13T11:15:32.334557Z","iopub.execute_input":"2023-10-13T11:15:32.334919Z","iopub.status.idle":"2023-10-13T11:15:32.339918Z","shell.execute_reply.started":"2023-10-13T11:15:32.334890Z","shell.execute_reply":"2023-10-13T11:15:32.339121Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom torchvision.models import vgg19_bn\nfrom torchvision.models.vgg import VGG19_BN_Weights\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-10-13T13:49:28.456859Z","iopub.execute_input":"2023-10-13T13:49:28.457212Z","iopub.status.idle":"2023-10-13T13:49:28.462672Z","shell.execute_reply.started":"2023-10-13T13:49:28.457183Z","shell.execute_reply":"2023-10-13T13:49:28.461823Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/input/covidct')","metadata":{"execution":{"iopub.status.busy":"2023-10-13T11:16:01.014514Z","iopub.execute_input":"2023-10-13T11:16:01.015038Z","iopub.status.idle":"2023-10-13T11:16:01.028524Z","shell.execute_reply.started":"2023-10-13T11:16:01.014986Z","shell.execute_reply":"2023-10-13T11:16:01.027365Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['CT_NonCOVID', 'COVID-CT-MetaInfo.xlsx', 'CT_COVID']"},"metadata":{}}]},{"cell_type":"code","source":"# 2. Get file paths and labels\npositive_samples = [os.path.join('/kaggle/input/covidct/CT_COVID', fname) for fname in os.listdir('/kaggle/input/covidct/CT_COVID')]\nnegative_samples = [os.path.join('/kaggle/input/covidct/CT_NonCOVID', fname) for fname in os.listdir('/kaggle/input/covidct/CT_NonCOVID')]\n\nall_samples = positive_samples + negative_samples\nlabels = [1] * len(positive_samples) + [0] * len(negative_samples)\n\n# 3. Perform stratified sampling\ntrain_samples, test_samples, train_labels, test_labels = train_test_split(all_samples, labels, test_size=0.2, stratify=labels)\n\n# Now, train_samples and test_samples contain the file paths for the training and testing datasets, respectively.\n# train_labels and test_labels contain the corresponding labels.\n","metadata":{"execution":{"iopub.status.busy":"2023-10-13T11:16:01.030231Z","iopub.execute_input":"2023-10-13T11:16:01.030651Z","iopub.status.idle":"2023-10-13T11:16:01.260409Z","shell.execute_reply.started":"2023-10-13T11:16:01.030621Z","shell.execute_reply":"2023-10-13T11:16:01.259534Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 2. Define custom datasets for the stratified data\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = CustomDataset(train_samples, train_labels, transform=transform)\ntest_dataset = CustomDataset(test_samples, test_labels, transform=transform)\n\n# 3. Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# 4. Load the VGG-19 model and modify for binary classification\n# model = models.vgg19_bn(pretrained=True)\nmodel = vgg19_bn(weights=VGG19_BN_Weights.IMAGENET1K_V1)\nfor param in model.parameters():\n    param.requires_grad = False  # Freeze all layers\n\nmodel.classifier[6] = nn.Linear(model.classifier[6].in_features, 1)  # Modify last layer for binary classification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# 5. Define the loss function, optimizer, and training loop\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.classifier[6].parameters(), lr=0.001)\n\ndef train_model(model, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n\n# 6. Train the model\ntrain_model(model, criterion, optimizer, num_epochs=50)\n\n# 7. Evaluate the model\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n        outputs = model(inputs)\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\npredicted_np = predicted.cpu().numpy()\nlabels_np = labels.cpu().numpy()\n\n# Calculate metrics\nprecision = precision_score(labels_np, predicted_np)\nrecall = recall_score(labels_np, predicted_np)\nf1 = f1_score(labels_np, predicted_np)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T13:56:52.346475Z","iopub.execute_input":"2023-10-13T13:56:52.346802Z","iopub.status.idle":"2023-10-13T14:03:12.813314Z","shell.execute_reply.started":"2023-10-13T13:56:52.346775Z","shell.execute_reply":"2023-10-13T14:03:12.812266Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/50 Loss: 0.6390\nEpoch 2/50 Loss: 0.5250\nEpoch 3/50 Loss: 0.4748\nEpoch 4/50 Loss: 0.4546\nEpoch 5/50 Loss: 0.4272\nEpoch 6/50 Loss: 0.4124\nEpoch 7/50 Loss: 0.4051\nEpoch 8/50 Loss: 0.3812\nEpoch 9/50 Loss: 0.3616\nEpoch 10/50 Loss: 0.4158\nEpoch 11/50 Loss: 0.3801\nEpoch 12/50 Loss: 0.3413\nEpoch 13/50 Loss: 0.3492\nEpoch 14/50 Loss: 0.3671\nEpoch 15/50 Loss: 0.3247\nEpoch 16/50 Loss: 0.3340\nEpoch 17/50 Loss: 0.3407\nEpoch 18/50 Loss: 0.3153\nEpoch 19/50 Loss: 0.3410\nEpoch 20/50 Loss: 0.3314\nEpoch 21/50 Loss: 0.3636\nEpoch 22/50 Loss: 0.3011\nEpoch 23/50 Loss: 0.3181\nEpoch 24/50 Loss: 0.3233\nEpoch 25/50 Loss: 0.3337\nEpoch 26/50 Loss: 0.3120\nEpoch 27/50 Loss: 0.3137\nEpoch 28/50 Loss: 0.3048\nEpoch 29/50 Loss: 0.3090\nEpoch 30/50 Loss: 0.3372\nEpoch 31/50 Loss: 0.3080\nEpoch 32/50 Loss: 0.2849\nEpoch 33/50 Loss: 0.3007\nEpoch 34/50 Loss: 0.2949\nEpoch 35/50 Loss: 0.2735\nEpoch 36/50 Loss: 0.2947\nEpoch 37/50 Loss: 0.2831\nEpoch 38/50 Loss: 0.3101\nEpoch 39/50 Loss: 0.2876\nEpoch 40/50 Loss: 0.2803\nEpoch 41/50 Loss: 0.2619\nEpoch 42/50 Loss: 0.2780\nEpoch 43/50 Loss: 0.3063\nEpoch 44/50 Loss: 0.2891\nEpoch 45/50 Loss: 0.3085\nEpoch 46/50 Loss: 0.2725\nEpoch 47/50 Loss: 0.2978\nEpoch 48/50 Loss: 0.3272\nEpoch 49/50 Loss: 0.2589\nEpoch 50/50 Loss: 0.2974\nTest Accuracy: 81.33%\nPrecision: 0.9000\nRecall: 0.6923\nF1 Score: 0.7826\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. Define custom datasets for the stratified data\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = CustomDataset(train_samples, train_labels, transform=transform)\ntest_dataset = CustomDataset(test_samples, test_labels, transform=transform)\n\n# 3. Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# 4. Load the VGG-19 model and modify for binary classification\n# model = models.vgg19_bn(pretrained=True)\nmodel = vgg19_bn(weights=VGG19_BN_Weights.IMAGENET1K_V1)\nfor param in model.parameters():\n    param.requires_grad = False  # Freeze all layers\n\nmodel.classifier[6] = nn.Linear(model.classifier[6].in_features, 1)  # Modify last layer for binary classification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# 5. Define the loss function, optimizer, and training loop\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.classifier[6].parameters(), lr=0.003)\n\ndef train_model(model, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n\n# 6. Train the model\ntrain_model(model, criterion, optimizer, num_epochs=50)\n\n# 7. Evaluate the model\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n        outputs = model(inputs)\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\npredicted_np = predicted.cpu().numpy()\nlabels_np = labels.cpu().numpy()\n\n# Calculate metrics\nprecision = precision_score(labels_np, predicted_np)\nrecall = recall_score(labels_np, predicted_np)\nf1 = f1_score(labels_np, predicted_np)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T14:03:12.815351Z","iopub.execute_input":"2023-10-13T14:03:12.815678Z","iopub.status.idle":"2023-10-13T14:09:33.771135Z","shell.execute_reply.started":"2023-10-13T14:03:12.815645Z","shell.execute_reply":"2023-10-13T14:09:33.770091Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/50 Loss: 0.6270\nEpoch 2/50 Loss: 0.5182\nEpoch 3/50 Loss: 0.4408\nEpoch 4/50 Loss: 0.4139\nEpoch 5/50 Loss: 0.4469\nEpoch 6/50 Loss: 0.3497\nEpoch 7/50 Loss: 0.3612\nEpoch 8/50 Loss: 0.3699\nEpoch 9/50 Loss: 0.3792\nEpoch 10/50 Loss: 0.3780\nEpoch 11/50 Loss: 0.3925\nEpoch 12/50 Loss: 0.3736\nEpoch 13/50 Loss: 0.3682\nEpoch 14/50 Loss: 0.3075\nEpoch 15/50 Loss: 0.3252\nEpoch 16/50 Loss: 0.3405\nEpoch 17/50 Loss: 0.3037\nEpoch 18/50 Loss: 0.3122\nEpoch 19/50 Loss: 0.3445\nEpoch 20/50 Loss: 0.3765\nEpoch 21/50 Loss: 0.3238\nEpoch 22/50 Loss: 0.2458\nEpoch 23/50 Loss: 0.2648\nEpoch 24/50 Loss: 0.3120\nEpoch 25/50 Loss: 0.2949\nEpoch 26/50 Loss: 0.3129\nEpoch 27/50 Loss: 0.3129\nEpoch 28/50 Loss: 0.2688\nEpoch 29/50 Loss: 0.3106\nEpoch 30/50 Loss: 0.3214\nEpoch 31/50 Loss: 0.2870\nEpoch 32/50 Loss: 0.2780\nEpoch 33/50 Loss: 0.2764\nEpoch 34/50 Loss: 0.2814\nEpoch 35/50 Loss: 0.2656\nEpoch 36/50 Loss: 0.2824\nEpoch 37/50 Loss: 0.3193\nEpoch 38/50 Loss: 0.2866\nEpoch 39/50 Loss: 0.3488\nEpoch 40/50 Loss: 0.3533\nEpoch 41/50 Loss: 0.2974\nEpoch 42/50 Loss: 0.2762\nEpoch 43/50 Loss: 0.2811\nEpoch 44/50 Loss: 0.2907\nEpoch 45/50 Loss: 0.3076\nEpoch 46/50 Loss: 0.2588\nEpoch 47/50 Loss: 0.3048\nEpoch 48/50 Loss: 0.2828\nEpoch 49/50 Loss: 0.2838\nEpoch 50/50 Loss: 0.2599\nTest Accuracy: 82.67%\nPrecision: 0.9000\nRecall: 0.6923\nF1 Score: 0.7826\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# 2. Define custom datasets for the stratified data\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = CustomDataset(train_samples, train_labels, transform=transform)\ntest_dataset = CustomDataset(test_samples, test_labels, transform=transform)\n\n# 3. Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# 4. Load the VGG-19 model and modify for binary classification\n# model = models.vgg19_bn(pretrained=True)\nmodel = vgg19_bn(weights=VGG19_BN_Weights.IMAGENET1K_V1)\nfor param in model.parameters():\n    param.requires_grad = True  # Freeze all layers\n\n# model.classifier[6] = nn.Linear(model.classifier[6].in_features, 1)  # Modify last layer for binary classification\n\n# Extract the original classifier layers of VGG-19\noriginal_classifier = model.classifier\n\n# Define new layers you want to add\nnew_layers = [\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(4096, 256),  # New dense layer with 256 neurons\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 1)    # Final layer for binary classification\n]\n\n# Extend the original classifier with new layers\nextended_classifier = nn.Sequential(\n    *list(original_classifier.children())[:-1],  # All layers except the last one\n    *new_layers                                 # Your new layers\n)\n\n# Replace the model's classifier with the new one\nmodel.classifier = extended_classifier\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# 5. Define the loss function, optimizer, and training loop\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n\ndef train_model(model, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n\n# 6. Train the model\ntrain_model(model, criterion, optimizer, num_epochs=10)\n\n# 7. Evaluate the model\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n        outputs = model(inputs)\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\npredicted_np = predicted.cpu().numpy()\nlabels_np = labels.cpu().numpy()\n\n# Calculate metrics\nprecision = precision_score(labels_np, predicted_np)\nrecall = recall_score(labels_np, predicted_np)\nf1 = f1_score(labels_np, predicted_np)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T14:09:33.772800Z","iopub.execute_input":"2023-10-13T14:09:33.773176Z","iopub.status.idle":"2023-10-13T14:12:11.489190Z","shell.execute_reply.started":"2023-10-13T14:09:33.773144Z","shell.execute_reply":"2023-10-13T14:12:11.488155Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/10 Loss: 0.8309\nEpoch 2/10 Loss: 0.5027\nEpoch 3/10 Loss: 0.2348\nEpoch 4/10 Loss: 0.2098\nEpoch 5/10 Loss: 0.1364\nEpoch 6/10 Loss: 0.1845\nEpoch 7/10 Loss: 0.0337\nEpoch 8/10 Loss: 0.1549\nEpoch 9/10 Loss: 0.2425\nEpoch 10/10 Loss: 0.0715\nTest Accuracy: 84.67%\nPrecision: 0.8000\nRecall: 0.9231\nF1 Score: 0.8571\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# 2. Define custom datasets for the stratified data\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = CustomDataset(train_samples, train_labels, transform=transform)\ntest_dataset = CustomDataset(test_samples, test_labels, transform=transform)\n\n# 3. Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# 4. Load the VGG-19 model and modify for binary classification\n# model = models.vgg19_bn(pretrained=True)\nmodel = vgg19_bn(weights=VGG19_BN_Weights.IMAGENET1K_V1)\nfor param in model.parameters():\n    param.requires_grad = True  # Freeze all layers\n\n# model.classifier[6] = nn.Linear(model.classifier[6].in_features, 1)  # Modify last layer for binary classification\n\n# Extract the original classifier layers of VGG-19\noriginal_classifier = model.classifier\n\n# Define new layers you want to add\nnew_layers = [\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(4096, 256),  # New dense layer with 256 neurons\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 1)    # Final layer for binary classification\n]\n\n# Extend the original classifier with new layers\nextended_classifier = nn.Sequential(\n    *list(original_classifier.children())[:-1],  # All layers except the last one\n    *new_layers                                 # Your new layers\n)\n\n# Replace the model's classifier with the new one\nmodel.classifier = extended_classifier\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# 5. Define the loss function, optimizer, and training loop\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n\ndef train_model(model, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n\n# 6. Train the model\ntrain_model(model, criterion, optimizer, num_epochs=30)\n\n# 7. Evaluate the model\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n        outputs = model(inputs)\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\npredicted_np = predicted.cpu().numpy()\nlabels_np = labels.cpu().numpy()\n\n# Calculate metrics\nprecision = precision_score(labels_np, predicted_np)\nrecall = recall_score(labels_np, predicted_np)\nf1 = f1_score(labels_np, predicted_np)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T14:12:11.491663Z","iopub.execute_input":"2023-10-13T14:12:11.491991Z","iopub.status.idle":"2023-10-13T14:19:57.395918Z","shell.execute_reply.started":"2023-10-13T14:12:11.491962Z","shell.execute_reply":"2023-10-13T14:19:57.394968Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/30 Loss: 0.9032\nEpoch 2/30 Loss: 0.5331\nEpoch 3/30 Loss: 0.3038\nEpoch 4/30 Loss: 0.1201\nEpoch 5/30 Loss: 0.1134\nEpoch 6/30 Loss: 0.1310\nEpoch 7/30 Loss: 0.2438\nEpoch 8/30 Loss: 0.0685\nEpoch 9/30 Loss: 0.1409\nEpoch 10/30 Loss: 0.1485\nEpoch 11/30 Loss: 0.0647\nEpoch 12/30 Loss: 0.0189\nEpoch 13/30 Loss: 0.0221\nEpoch 14/30 Loss: 0.0711\nEpoch 15/30 Loss: 0.0912\nEpoch 16/30 Loss: 0.2015\nEpoch 17/30 Loss: 0.0348\nEpoch 18/30 Loss: 0.0502\nEpoch 19/30 Loss: 0.0636\nEpoch 20/30 Loss: 0.0750\nEpoch 21/30 Loss: 0.0518\nEpoch 22/30 Loss: 0.0768\nEpoch 23/30 Loss: 0.0540\nEpoch 24/30 Loss: 0.0176\nEpoch 25/30 Loss: 0.0553\nEpoch 26/30 Loss: 0.1029\nEpoch 27/30 Loss: 0.0212\nEpoch 28/30 Loss: 0.1344\nEpoch 29/30 Loss: 0.0768\nEpoch 30/30 Loss: 0.0155\nTest Accuracy: 84.67%\nPrecision: 1.0000\nRecall: 0.6923\nF1 Score: 0.8182\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# 2. Define custom datasets for the stratified data\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = CustomDataset(train_samples, train_labels, transform=transform)\ntest_dataset = CustomDataset(test_samples, test_labels, transform=transform)\n\n# 3. Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# 4. Load the VGG-19 model and modify for binary classification\n# model = models.vgg19_bn(pretrained=True)\nmodel = vgg19_bn(weights=VGG19_BN_Weights.IMAGENET1K_V1)\nfor param in model.parameters():\n    param.requires_grad = True  # Freeze all layers\n\n# model.classifier[6] = nn.Linear(model.classifier[6].in_features, 1)  # Modify last layer for binary classification\n\n# Extract the original classifier layers of VGG-19\noriginal_classifier = model.classifier\n\n# Define new layers you want to add\nnew_layers = [\n    nn.Linear(4096, 512),\n    nn.BatchNorm1d(512),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    \n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),\n    nn.LeakyReLU(0.1),\n    nn.Dropout(0.4),\n    \n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    \n    nn.Linear(128, 1)\n]\n\n# Extend the original classifier with new layers\nextended_classifier = nn.Sequential(\n    *list(original_classifier.children())[:-1],  # All layers except the last one\n    *new_layers                                 # Your new layers\n)\n\n# Replace the model's classifier with the new one\nmodel.classifier = extended_classifier\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# 5. Define the loss function, optimizer, and training loop\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n\ndef train_model(model, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n\n# 6. Train the model\ntrain_model(model, criterion, optimizer, num_epochs=30)\n\n# 7. Evaluate the model\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n        outputs = model(inputs)\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        \npredicted_np = predicted.cpu().numpy()\nlabels_np = labels.cpu().numpy()\n\n# Calculate metrics\nprecision = precision_score(labels_np, predicted_np)\nrecall = recall_score(labels_np, predicted_np)\nf1 = f1_score(labels_np, predicted_np)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T14:19:57.397175Z","iopub.execute_input":"2023-10-13T14:19:57.397489Z","iopub.status.idle":"2023-10-13T14:27:40.975781Z","shell.execute_reply.started":"2023-10-13T14:19:57.397459Z","shell.execute_reply":"2023-10-13T14:27:40.974824Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/30 Loss: 0.6730\nEpoch 2/30 Loss: 0.5109\nEpoch 3/30 Loss: 0.2933\nEpoch 4/30 Loss: 0.1664\nEpoch 5/30 Loss: 0.0822\nEpoch 6/30 Loss: 0.0404\nEpoch 7/30 Loss: 0.0274\nEpoch 8/30 Loss: 0.0354\nEpoch 9/30 Loss: 0.0248\nEpoch 10/30 Loss: 0.0674\nEpoch 11/30 Loss: 0.0798\nEpoch 12/30 Loss: 0.0773\nEpoch 13/30 Loss: 0.0497\nEpoch 14/30 Loss: 0.0523\nEpoch 15/30 Loss: 0.0217\nEpoch 16/30 Loss: 0.0232\nEpoch 17/30 Loss: 0.0228\nEpoch 18/30 Loss: 0.0407\nEpoch 19/30 Loss: 0.0377\nEpoch 20/30 Loss: 0.0216\nEpoch 21/30 Loss: 0.0201\nEpoch 22/30 Loss: 0.0434\nEpoch 23/30 Loss: 0.0283\nEpoch 24/30 Loss: 0.0092\nEpoch 25/30 Loss: 0.0087\nEpoch 26/30 Loss: 0.0171\nEpoch 27/30 Loss: 0.0123\nEpoch 28/30 Loss: 0.0276\nEpoch 29/30 Loss: 0.0151\nEpoch 30/30 Loss: 0.0198\nTest Accuracy: 88.67%\nPrecision: 0.9167\nRecall: 0.8462\nF1 Score: 0.8800\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# 2. Define custom datasets for the stratified data\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = CustomDataset(train_samples, train_labels, transform=transform)\ntest_dataset = CustomDataset(test_samples, test_labels, transform=transform)\n\n# 3. Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# 4. Load the VGG-19 model and modify for binary classification\n# model = models.vgg19_bn(pretrained=True)\nmodel = vgg19_bn(weights=VGG19_BN_Weights.IMAGENET1K_V1)\nfor param in model.parameters():\n    param.requires_grad = True  # Freeze all layers\n\n# model.classifier[6] = nn.Linear(model.classifier[6].in_features, 1)  # Modify last layer for binary classification\n\n# Extract the original classifier layers of VGG-19\noriginal_classifier = model.classifier\n\n# Define new layers you want to add\nnew_layers = [\n    nn.Linear(4096, 512),\n    nn.BatchNorm1d(512),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    \n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),\n    nn.LeakyReLU(0.1),\n    nn.Dropout(0.4),\n    \n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    \n    nn.Linear(128, 1)\n]\n\n# Extend the original classifier with new layers\nextended_classifier = nn.Sequential(\n    *list(original_classifier.children())[:-1],  # All layers except the last one\n    *new_layers                                 # Your new layers\n)\n\n# Replace the model's classifier with the new one\nmodel.classifier = extended_classifier\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# 5. Define the loss function, optimizer, and training loop\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n\n# Early stopping parameters\nn_epochs_stop = 5  # Number of epochs with no improvement after which training will be stopped\nmin_val_loss = float('inf')\nepochs_no_improve = 0\n\ndef train_model(model, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n        \n        # Validation loss for the epoch\n        val_loss = 0\n        model.eval()\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * inputs.size(0)\n\n        val_loss /= len(val_loader.dataset)\n\n        # Check if the validation loss has improved\n        if val_loss < min_val_loss:\n            min_val_loss = val_loss\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve == n_epochs_stop:\n                print(f\"Early stopping! Stopped at epoch {epoch}\")\n                break\n\n\n# 6. Train the model\ntrain_model(model, criterion, optimizer, num_epochs=30)\n\n# 7. Evaluate the model\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n        outputs = model(inputs)\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        \npredicted_np = predicted.cpu().numpy()\nlabels_np = labels.cpu().numpy()\n\n# Calculate metrics\nprecision = precision_score(labels_np, predicted_np)\nrecall = recall_score(labels_np, predicted_np)\nf1 = f1_score(labels_np, predicted_np)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T14:27:40.977002Z","iopub.execute_input":"2023-10-13T14:27:40.977339Z","iopub.status.idle":"2023-10-13T14:27:58.124752Z","shell.execute_reply.started":"2023-10-13T14:27:40.977292Z","shell.execute_reply":"2023-10-13T14:27:58.123360Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch 1/30 Loss: 0.6544\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 128\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# 6. Train the model\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 7. Evaluate the model\u001b[39;00m\n\u001b[1;32m    131\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","Cell \u001b[0;32mIn[27], line 108\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m    106\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_loader\u001b[49m:\n\u001b[1;32m    109\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    110\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n","\u001b[0;31mNameError\u001b[0m: name 'val_loader' is not defined"],"ename":"NameError","evalue":"name 'val_loader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}